{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a6e9114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import from nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63777455",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n1/w0jj77mn7yv1tywxf17vqh300000gn/T/ipykernel_31092/621171849.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Word tokenise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(tokens)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# tag some text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# Word tokenise\n",
    "tokens = word_tokenize(text)\n",
    "# print(tokens)\n",
    "# tag some text\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged[:5])\n",
    "POS = {\n",
    "    \"CD\": \"cardinal numbers\",\n",
    "    \"AT\": \"articles\",\n",
    "    \"JJ\": \"adjectives\",\n",
    "    \"NN\": \"nouns formed from oadjectives & nouns (default)\",\n",
    "    \"RB\": \"adverbs\",\n",
    "    \"NNS\": \"plural nouns\",\n",
    "    \"VBG\": \"gerunds\",\n",
    "    \"VBD\": \"past tense verbs\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c60fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(w) for w in tokens]\n",
    "print(\" \".join(stemmed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a0d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizing\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmatized = [wnl.lemmatize(w) for w in tokens]\n",
    "print(\" \".join(lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopword removal\n",
    "sw = stopwords.words(\"English\")\n",
    "content = [w for w in tokens if w.lower() not in sw]\n",
    "\" \".join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de0219",
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets = wordnet.synsets('phone')\n",
    "\n",
    "for synset in synsets:\n",
    "    print(synset.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51bb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets = wordnet.synsets('cell')\n",
    "\n",
    "for synset in synsets:\n",
    "    print(synset.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e164d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "synsets = wordnet.synsets('mobile phone')\n",
    "\n",
    "for synset in synsets:\n",
    "    print(synset.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db54a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "phone = wordnet.synsets('car')\n",
    "synsets = phone[0].hyponyms()\n",
    "for synset in synsets:\n",
    "    print(synset.lemma_names())\n",
    "    print(synset.definition())\n",
    "    print(synset.examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c8beb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56edeaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076111e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "antonyms = []\n",
    "\n",
    "for syn in wordnet.synsets('good'):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.extend([n.name() for n in l.antonyms()])\n",
    "            \n",
    "print(set(synonyms), set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fa1ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarities\n",
    "\n",
    "w1 = wordnet.synset(\"agriculture.n.01\")\n",
    "w2 = wordnet.synset(\"farm.n.01\")\n",
    "print(w1.wup_similarity(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0df2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33d7c8c4",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc22e81b",
   "metadata": {},
   "source": [
    "https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2e11b3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.0.0/en_core_web_lg-3.0.0-py3-none-any.whl (778.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 778.8 MB 33 kB/s  eta 0:00:01    |                                | 2.3 MB 10.8 MB/s eta 0:01:13     |██████                          | 145.6 MB 4.2 MB/s eta 0:02:30��████                       | 217.3 MB 6.1 MB/s eta 0:01:32     |██████████████████████          | 536.5 MB 4.1 MB/s eta 0:01:00     |████████████████████████████▏   | 686.0 MB 4.1 MB/s eta 0:00:23\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from en-core-web-lg==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (4.61.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: jinja2 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (3.0.1)\n",
      "Requirement already satisfied: setuptools in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (8.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.21.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (0.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (20.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (5.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (1.26.6)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/scchung/opt/anaconda3/envs/test-ibm/lib/python3.9/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-lg==3.0.0) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "# # !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_md\n",
    "!python -m spacy download en_core_web_lg\n",
    "\n",
    "# !pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ae7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "import spacy\n",
    "\n",
    "nlp = English()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e2a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"its official Apple is the first U.S. public company to reach 1 trillion market value\")\n",
    "# for token in doc:\n",
    "#     print(token.text, token.pos_, token.dep_) # token's text, part-of-speech tag, dependency label\n",
    "    \n",
    "# entity text, label\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c32c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "doc1 = nlp(\"irrigation\")\n",
    "doc2 = nlp(\"water\")\n",
    "\n",
    "# doc1 = nlp(\"tired\")\n",
    "# doc2 = nlp(\"exhausted\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c36a33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt') as f:\n",
    "    transcript = f.read()\n",
    "    \n",
    "subtitle = Subtitle(transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade60f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6891da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "doc = nlp(text)\n",
    "html = displacy.render(doc, style=\"ent\", jupyter=False)\n",
    "\n",
    "html\n",
    "# with open('data_vis.html', 'w') as f:\n",
    "#     f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27b27eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_vis.html', 'w') as f:\n",
    "    f.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b17e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c31520c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fec807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspellchecker\n",
    "# https://github.com/barrust/pyspellchecker\n",
    "from spellchecker import SpellChecker\n",
    "from caption import pipeline\n",
    "\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59084db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd93a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de610c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_first100 = tokens[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97e86bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find words that may be misspelled\n",
    "misspelled = spell.unknown(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0567feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df65d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell.correction('tpescrupt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb50f095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cb615e3",
   "metadata": {},
   "source": [
    "## Training spell checker with text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "98e018d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenize, lemmatize and stop word removal\n",
    "# import from nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def process_text(text):\n",
    "    # Word tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Lemmatize\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemmatized = [wnl.lemmatize(w) for w in tokens]\n",
    "    # Stop word removal\n",
    "    sw = stopwords.words(\"English\")\n",
    "    content = [w for w in tokens if w.lower() not in sw]\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2894aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spell check\n",
    "# pyspellchecker\n",
    "# https://github.com/barrust/pyspellchecker\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "from caption import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0b7c6b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "03cdfa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YoutubeCaption created\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'process_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n1/w0jj77mn7yv1tywxf17vqh300000gn/T/ipykernel_31331/4276910693.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mspell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspellchecker_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/n1/w0jj77mn7yv1tywxf17vqh300000gn/T/ipykernel_31331/4276910693.py\u001b[0m in \u001b[0;36mspellchecker_load\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mspell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpellChecker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# process text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# add text to word frequency list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mspell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_frequency\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'process_text' is not defined"
     ]
    }
   ],
   "source": [
    "url = 'https://www.youtube.com/watch?v=Ata9cSC2WpM'\n",
    "text = pipeline(url)['caption_fulltext']\n",
    "\n",
    "def spellchecker_load(text):\n",
    "    # load spell checked\n",
    "    spell = SpellChecker()\n",
    "    # process text\n",
    "    tokens = process_text(text)\n",
    "    # add text to word frequency list\n",
    "    spell.word_frequency.load_words(tokens)\n",
    "    # return model\n",
    "    return spell\n",
    "\n",
    "spell = spellchecker_load(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142fc556",
   "metadata": {},
   "source": [
    "e.g. tokens: ['angular',\n",
    " 'typescript-based',\n",
    " 'framework',\n",
    " 'building',\n",
    " 'user',\n",
    " 'interfaces',\n",
    " 'developed',\n",
    " 'google',\n",
    " 'released',\n",
    " '2016',\n",
    " 'sequel',\n",
    " 'angularjs',\n",
    " 'angular',\n",
    " 'developer', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "21db6a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'typrscript-baed'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.correction('typrscript-baed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae01a510",
   "metadata": {},
   "source": [
    "## word search:\n",
    "1. [x] strip to remove leading and trailing space\n",
    "1. [x] regex to remove punctuation from either side\n",
    "1. if search contains a single word (only alphanumerics)\n",
    "    1. lemmatize word and match exact lemmatized token\n",
    "    1. find synonyms and match synonym token\n",
    "    1. find token with similarity > 0.7\n",
    "    1. find potentially mispelled search word\n",
    "1. [x] if search contains multiple words\n",
    "    1. replace all punctuation with space\n",
    "    1. replace multiples of space to single space\n",
    "    1. search text for exact match with \\[0 to 5\\] characters between each word\n",
    "    1. * alternatives: chunking, entity recognition\n",
    "       * limitations: punctuation, missing mispelled/ similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ccfb7bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n1/w0jj77mn7yv1tywxf17vqh300000gn/T/ipykernel_31331/2660773619.py:30: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  similarity = doc1.similarity(doc2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'spell_correction': True,\n",
       " 'results': [{'match': 'typescript', 'type': 'exact'},\n",
       "  {'match': 'typescript', 'type': 'exact'}]}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "import spacy\n",
    "\n",
    "# get synonyms for word\n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.add(l.name())\n",
    "    return synonyms\n",
    "\n",
    "def word_matching(q, text):\n",
    "    matches = []\n",
    "#     tokenize text\n",
    "    tokens = word_tokenize(text)\n",
    "#     set up lemmatizer\n",
    "    wnl = WordNetLemmatizer()\n",
    "# get synonyms\n",
    "    synonyms = get_synonyms('good')\n",
    "#     load spacy\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "# loop through tokens\n",
    "    for token in tokens:\n",
    "#         calculate similarity score\n",
    "        doc1, doc2 = nlp(q), nlp(token)\n",
    "        similarity = doc1.similarity(doc2)\n",
    "\n",
    "#     match exact\n",
    "        if token == q:\n",
    "            matches.append({'match': token, 'type': 'exact'})\n",
    "#     match synonyms\n",
    "        elif wnl.lemmatize(token) in get_synonyms(q):\n",
    "            matches.append({'match': token, 'type': 'synonym'})\n",
    "# match high similarity words\n",
    "        elif similarity > 0.8:\n",
    "            matches.append({'match': token, 'type': 'similarity', 'score': similarity})\n",
    "    return matches\n",
    "\n",
    "def match_single_word(q, text):\n",
    "    matches = word_matching(q, text)\n",
    "    #     check for misspelled query word if no results are found\n",
    "    if matches == []:\n",
    "#         load spell checker\n",
    "        spell = spellchecker_load(text)\n",
    "        if spell.correction(q) != q:\n",
    "            results = {'spell_correction': True,'matches': word_matching(spell.correction(q), text)}\n",
    "    else:\n",
    "        results = {'spell_correction': False, 'matches': matches}\n",
    "    return results\n",
    "\n",
    "# for token in tokens:\n",
    "#     if wnl.lemmatize(token) == q:\n",
    "#         matches.add(token)\n",
    "\n",
    "# for match in matches:\n",
    "#     results = re.finditer(match, text)\n",
    "#     results = [{'start': m.start(), 'end': m.end(), 'match': m.group(0)} for m in results]\n",
    "#     res = {'status': 400, 'results': results}\n",
    "\n",
    "# print(results)\n",
    "\n",
    "# q = 'typescripts'\n",
    "# synonyms = get_synonyms(q)\n",
    "# print(synonyms)\n",
    "# q = wnl.lemmatize(q)\n",
    "\n",
    "\n",
    "# for r in results:\n",
    "#     print(sections[find_section(r['start'], sections) - 1])\n",
    "    \n",
    "# query = nlp(q)\n",
    "# for token in tokens:\n",
    "#     score = query.similarity(nlp(token))\n",
    "#     if score > 0.8:\n",
    "#         print(score, token)\n",
    "match_single_word('typescpt', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4558585b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"angular a typescript-based framework for building user interfaces it was developed at google and released in 2016 as the sequel to angularjs as an angular developer you hit the ground running with its extremely powerful cli tool when you generate your initial application it comes pre-configured with routing a testing framework and your favorite style preprocessor in addition the magic ng add command can turn your app into a progressive web app add server-side rendering firebase support and do a whole bunch of other cool stuff but at its core angular is just a component-based ui library i can create a component with the cli and if we go into its typescript file you'll notice the component decorator which makes this typescript class a component any properties on this class are considered reactive state and when their values change the component will re-render the ui for example we can bind the property to html using double braces in the template from there we can add a button that increments this value every time it's clicked we add the event name on the left side in parentheses then an expression on the right side in this case it points to a method in our class each time the button is clicked it calls the method which changes the state and re-renders the ui angular also has a variety of directives for building complex templates use ngf to handle conditional logic or if you have an iterable value use ng4 to loop over it but where angular really excels is handling complexity and one of its primary tools for doing so is called dependency injection when your app grows to hundreds of components you'll likely need a way to share data and functionality between them we can take our component logic here and extract it into a service which can be treated as a global singleton throughout the application now any component that wants to use this state or logic can simply add this class to its constructor the end result is a simple and reliable way to compose complex applications as a developer you can always count on a consistent experience between projects and minimal decision fatigue this has been angular in 100 seconds if you want to see more short videos like this make sure to like and subscribe and check out a ton of advanced angular content on fireship io thanks for watching and i will see you in the next one\""
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92ce8884",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YoutubeCaption created\n"
     ]
    }
   ],
   "source": [
    "from captiojn import pipeline\n",
    "\n",
    "caption = pipeline('https://www.youtube.com/watch?v=Ata9cSC2WpM')\n",
    "text = caption['caption_fulltext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4bec0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sum([len(section['subtitle']) for section in sections]) + len(sections) - 1)\n",
    "# print(len(text))\n",
    "\n",
    "# cid = [len(sections[0]['subtitle'])]\n",
    "           \n",
    "# for index, section in enumerate(sections[1:]):\n",
    "#     cid.append(cid[index] + len(section['subtitle']) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "996fc63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b67f2c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search sectioning helper code\n",
    "\n",
    "import json\n",
    "sections = json.loads(caption['caption_sections'])\n",
    "# create cumulative index column for each section\n",
    "sections[0]['cid'] = len(sections[0]['subtitle'])\n",
    "# cid: space offset + previous section's cid + number of characters in the previous section's subtitle\n",
    "for index, section in enumerate(sections[1:]):\n",
    "    section['cid'] = 1 + sections[index]['cid'] + len(sections[index]['subtitle'])\n",
    "\n",
    "# loop through sections to find where the term starts\n",
    "def find_section(startid, sections):\n",
    "    for section in sections:\n",
    "        if startid < section['cid']:\n",
    "            return int(section['section'] )\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "531b0abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_search_term(search):\n",
    "#    remove leading and trailing space and punctuation\n",
    "#     search = search.strip()\n",
    "    search = re.sub(r'^\\W*(.*?)\\W*$', r'\\1', search)\n",
    "#     return alphanumeric tokens\n",
    "    tokens = re.findall(r'(\\w+)', search)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcc14c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['typescript', 'based']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'typescript based'\n",
    "search_tokens = process_search_term(query)\n",
    "search_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71f406b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': 400, 'results': [{'start': 10, 'end': 26, 'match': 'typescript-based'}]}\n"
     ]
    }
   ],
   "source": [
    "res = {}\n",
    "\n",
    "if len(search_tokens) == 0:\n",
    "    res = {'status': 400, 'message': \"No search term parsed\"}\n",
    "elif len(search_tokens) == 1:\n",
    "    pass\n",
    "else:\n",
    "    search = r'.{0,3}'.join(search_tokens)\n",
    "    results = re.finditer(search, text)\n",
    "    results = [{'start': m.start(), 'end': m.end(), 'match': m.group(0)} for m in results]\n",
    "    res = {'status': 400, 'results': results}\n",
    "    \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f714caa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'section': '1', 'time_original': '00:00:00,320 --> 00:00:04,160', 'time': '0.32:4.16', 'subtitle': 'angular a typescript-based framework for', 'cid': 40}\n"
     ]
    }
   ],
   "source": [
    "for r in results:\n",
    "    print(sections[find_section(r['start'], sections) - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107a686e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffa42a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9ba441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61225ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deb921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.compile(('.').join(search_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88272ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'\\d{1,3}', '00  33')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6ba801e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'start'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n1/w0jj77mn7yv1tywxf17vqh300000gn/T/ipykernel_31206/1874473410.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'start'"
     ]
    }
   ],
   "source": [
    "for match in results:\n",
    "    print(match.start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3f9ae47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10353"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a96fea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
