{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPZ4hdSrn0Y/DGpqFDN3lWW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monalizards/technical_video_search_assistant/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x056rsEBOZeK",
        "outputId": "467a37b1-e394-40c2-af4e-0442b347f803"
      },
      "source": [
        "!pip install pytube"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytube\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/d0/01b6936455527bdf7cf32f0fd32f7365bc7e3aa066ff3b16d45fa42c71f1/pytube-10.8.4-py3-none-any.whl (46kB)\n",
            "\r\u001b[K     |███████                         | 10kB 20.0MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 20kB 16.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 30kB 14.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 40kB 13.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.1MB/s \n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-10.8.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQS0HUv_O0tz",
        "outputId": "d21a1120-4a3c-4fc9-f778-0d003943794f"
      },
      "source": [
        "!pip install moviepy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (0.2.3.5)\n",
            "Requirement already satisfied: imageio<3.0,>=2.1.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (2.4.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from moviepy) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.7/dist-packages (from moviepy) (4.41.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio<3.0,>=2.1.2->moviepy) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqgUYCmvPGtZ",
        "outputId": "98abb5ae-498f-4abd-a83e-5ef021a75202"
      },
      "source": [
        "!pip install ibm_watson ibm_cloud_sdk_core"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ibm_watson\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/43/13fa3cc7df4f9eb0e55748087cc6b8042d6e0399ca3f4ecd650c6c2d9f92/ibm-watson-5.2.0.tar.gz (409kB)\n",
            "\r\u001b[K     |▉                               | 10kB 12.6MB/s eta 0:00:01\r\u001b[K     |█▋                              | 20kB 18.7MB/s eta 0:00:01\r\u001b[K     |██▍                             | 30kB 23.3MB/s eta 0:00:01\r\u001b[K     |███▏                            | 40kB 21.6MB/s eta 0:00:01\r\u001b[K     |████                            | 51kB 16.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 61kB 12.5MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 71kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 81kB 13.9MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 92kB 13.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 102kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 112kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 122kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 133kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 143kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 153kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 163kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 174kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 184kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 194kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████                | 204kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 215kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 225kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 235kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 245kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 256kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 266kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 276kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 286kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 296kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 307kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 317kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 327kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 337kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 348kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 358kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 368kB 14.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 378kB 14.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 389kB 14.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 399kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 409kB 14.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 419kB 14.2MB/s \n",
            "\u001b[?25hCollecting ibm_cloud_sdk_core\n",
            "  Downloading https://files.pythonhosted.org/packages/99/55/ece4d000ca41c052c7331a0d14150e9c9c15e4f65943036cfff3bcd14cc7/ibm-cloud-sdk-core-3.10.0.tar.gz\n",
            "Requirement already satisfied: requests<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from ibm_watson) (2.23.0)\n",
            "Requirement already satisfied: python_dateutil>=2.5.3 in /usr/local/lib/python3.7/dist-packages (from ibm_watson) (2.8.1)\n",
            "Collecting websocket-client==0.48.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/a1/72ef9aa26cfe1a75cee09fc1957e4723add9de098c15719416a1ee89386b/websocket_client-0.48.0-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 26.6MB/s \n",
            "\u001b[?25hCollecting PyJWT<3.0.0,>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/3f/32/d5d3cab27fee7f6b22d7cd7507547ae45d52e26030fa77d1f83d0526c6e5/PyJWT-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.0->ibm_watson) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.0->ibm_watson) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.0->ibm_watson) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.0->ibm_watson) (2.10)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python_dateutil>=2.5.3->ibm_watson) (1.15.0)\n",
            "Building wheels for collected packages: ibm-watson, ibm-cloud-sdk-core\n",
            "  Building wheel for ibm-watson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-watson: filename=ibm_watson-5.2.0-cp37-none-any.whl size=403252 sha256=599c4dffdf2397d1242a8aedb0620ae9cad5ea77ab92be669d332991bb0fd399\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/e1/8c/592f08568441aee6d1c9b61bc7a425b509e0607beb306ebf69\n",
            "  Building wheel for ibm-cloud-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ibm-cloud-sdk-core: filename=ibm_cloud_sdk_core-3.10.0-cp37-none-any.whl size=60922 sha256=59778ff4196d1cd9d5eb6cdb6ced79ae9f17f152e4e88bbc240d7a5549fa2b0a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/4e/48/b02ad6dc75235fc4c0742d4e99571fe7d729e60bf365105be4\n",
            "Successfully built ibm-watson ibm-cloud-sdk-core\n",
            "Installing collected packages: websocket-client, PyJWT, ibm-cloud-sdk-core, ibm-watson\n",
            "Successfully installed PyJWT-2.1.0 ibm-cloud-sdk-core-3.10.0 ibm-watson-5.2.0 websocket-client-0.48.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qow_pM8KPMkx"
      },
      "source": [
        "apikey = 'PvFDc759NRCbfBhRgKlqi87QsDl7kpNvQYkJZTaEfGCA'\n",
        "url = 'https://api.eu-gb.speech-to-text.watson.cloud.ibm.com/instances/33ecdf1d-e4a5-4521-83e6-d3c3c574d9b0'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf4qQkOCPhIa"
      },
      "source": [
        "from pytube import YouTube"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txlz40CWPQb6"
      },
      "source": [
        "link = 'https://www.youtube.com/watch?v=RLYoEyIHL6A'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGkJ9iatPUZM",
        "outputId": "98a4633d-5706-4413-84fe-8ae6f137c219"
      },
      "source": [
        "yt = YouTube(link)\n",
        "print(yt.title)\n",
        "captions = yt.captions['a.en'].generate_srt_captions()\n",
        "print(captions)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Google Colab Tutorial for Beginners | Get Started with Google Colab\n",
            "1\n",
            "00:00:00,080 --> 00:00:04,000\n",
            "google collaboratory or caller for short\n",
            "\n",
            "2\n",
            "00:00:02,480 --> 00:00:06,080\n",
            "is the best way to get started with\n",
            "\n",
            "3\n",
            "00:00:04,000 --> 00:00:07,440\n",
            "artificial intelligence and data science\n",
            "\n",
            "4\n",
            "00:00:06,080 --> 00:00:09,200\n",
            "everything you need all the major\n",
            "\n",
            "5\n",
            "00:00:07,440 --> 00:00:11,040\n",
            "libraries and tools are already\n",
            "\n",
            "6\n",
            "00:00:09,200 --> 00:00:11,360\n",
            "pre-installed for you and pre-set up for\n",
            "\n",
            "7\n",
            "00:00:11,040 --> 00:00:13,599\n",
            "you\n",
            "\n",
            "8\n",
            "00:00:11,360 --> 00:00:15,519\n",
            "so by the time it takes you to search\n",
            "\n",
            "9\n",
            "00:00:13,599 --> 00:00:16,080\n",
            "how to install another software you can\n",
            "\n",
            "10\n",
            "00:00:15,519 --> 00:00:18,400\n",
            "already be\n",
            "\n",
            "11\n",
            "00:00:16,080 --> 00:00:20,480\n",
            "get up and running coding your data\n",
            "\n",
            "12\n",
            "00:00:18,400 --> 00:00:21,920\n",
            "science and artificial intelligence code\n",
            "\n",
            "13\n",
            "00:00:20,480 --> 00:00:24,480\n",
            "with google call up you can\n",
            "\n",
            "14\n",
            "00:00:21,920 --> 00:00:26,000\n",
            "write run and share python code in your\n",
            "\n",
            "15\n",
            "00:00:24,480 --> 00:00:28,240\n",
            "browser and this allows you to get\n",
            "\n",
            "16\n",
            "00:00:26,000 --> 00:00:29,920\n",
            "started from your phone tablet or laptop\n",
            "\n",
            "17\n",
            "00:00:28,240 --> 00:00:32,079\n",
            "without any configuration and you can\n",
            "\n",
            "18\n",
            "00:00:29,920 --> 00:00:33,120\n",
            "immediately share your code with anyone\n",
            "\n",
            "19\n",
            "00:00:32,079 --> 00:00:34,800\n",
            "with a simple link\n",
            "\n",
            "20\n",
            "00:00:33,120 --> 00:00:36,480\n",
            "and everything you do in call app will\n",
            "\n",
            "21\n",
            "00:00:34,800 --> 00:00:36,960\n",
            "be saved under your google account so\n",
            "\n",
            "22\n",
            "00:00:36,480 --> 00:00:38,640\n",
            "you can\n",
            "\n",
            "23\n",
            "00:00:36,960 --> 00:00:40,399\n",
            "access them share them just like the\n",
            "\n",
            "24\n",
            "00:00:38,640 --> 00:00:41,840\n",
            "rest of your files under your google\n",
            "\n",
            "25\n",
            "00:00:40,399 --> 00:00:43,680\n",
            "drive if you are familiar with the\n",
            "\n",
            "26\n",
            "00:00:41,840 --> 00:00:44,399\n",
            "jupyter notebooks google collab is\n",
            "\n",
            "27\n",
            "00:00:43,680 --> 00:00:46,320\n",
            "essentially\n",
            "\n",
            "28\n",
            "00:00:44,399 --> 00:00:48,480\n",
            "a jupiter notebook hosted on google\n",
            "\n",
            "29\n",
            "00:00:46,320 --> 00:00:50,399\n",
            "servers with additional functionalities\n",
            "\n",
            "30\n",
            "00:00:48,480 --> 00:00:52,000\n",
            "that makes it especially popular among\n",
            "\n",
            "31\n",
            "00:00:50,399 --> 00:00:52,719\n",
            "data science artificial intelligence\n",
            "\n",
            "32\n",
            "00:00:52,000 --> 00:00:54,399\n",
            "communities\n",
            "\n",
            "33\n",
            "00:00:52,719 --> 00:00:56,079\n",
            "so without further ado let's go ahead\n",
            "\n",
            "34\n",
            "00:00:54,399 --> 00:00:56,879\n",
            "and get started with call up so first\n",
            "\n",
            "35\n",
            "00:00:56,079 --> 00:00:59,039\n",
            "thing you want to do\n",
            "\n",
            "36\n",
            "00:00:56,879 --> 00:01:01,320\n",
            "is to search for google call up and it\n",
            "\n",
            "37\n",
            "00:00:59,039 --> 00:01:02,879\n",
            "should be the one that says\n",
            "\n",
            "38\n",
            "00:01:01,320 --> 00:01:04,080\n",
            "callup.research.google.com let's go into\n",
            "\n",
            "39\n",
            "00:01:02,879 --> 00:01:05,680\n",
            "that\n",
            "\n",
            "40\n",
            "00:01:04,080 --> 00:01:07,760\n",
            "so the very first thing you should know\n",
            "\n",
            "41\n",
            "00:01:05,680 --> 00:01:10,320\n",
            "about google call up is that everything\n",
            "\n",
            "42\n",
            "00:01:07,760 --> 00:01:11,439\n",
            "is organized under notebooks so first\n",
            "\n",
            "43\n",
            "00:01:10,320 --> 00:01:12,159\n",
            "thing you see when you come to the\n",
            "\n",
            "44\n",
            "00:01:11,439 --> 00:01:14,080\n",
            "website\n",
            "\n",
            "45\n",
            "00:01:12,159 --> 00:01:15,680\n",
            "are basically your recent notebooks and\n",
            "\n",
            "46\n",
            "00:01:14,080 --> 00:01:17,520\n",
            "the first one you will see will probably\n",
            "\n",
            "47\n",
            "00:01:15,680 --> 00:01:18,960\n",
            "be the welcome to collaborator notebook\n",
            "\n",
            "48\n",
            "00:01:17,520 --> 00:01:20,880\n",
            "so to get started first thing you want\n",
            "\n",
            "49\n",
            "00:01:18,960 --> 00:01:23,200\n",
            "to do is to go ahead and create a new\n",
            "\n",
            "50\n",
            "00:01:20,880 --> 00:01:25,119\n",
            "notebook\n",
            "\n",
            "51\n",
            "00:01:23,200 --> 00:01:26,880\n",
            "so the first time you open a collab\n",
            "\n",
            "52\n",
            "00:01:25,119 --> 00:01:29,520\n",
            "notebook it will name it for you\n",
            "\n",
            "53\n",
            "00:01:26,880 --> 00:01:30,079\n",
            "with untitled 0 in this case it is 3 for\n",
            "\n",
            "54\n",
            "00:01:29,520 --> 00:01:31,759\n",
            "me\n",
            "\n",
            "55\n",
            "00:01:30,079 --> 00:01:33,759\n",
            "and one thing you want to do is to\n",
            "\n",
            "56\n",
            "00:01:31,759 --> 00:01:35,200\n",
            "rename this notebook so that it's more\n",
            "\n",
            "57\n",
            "00:01:33,759 --> 00:01:36,640\n",
            "meaningful to us it can be something\n",
            "\n",
            "58\n",
            "00:01:35,200 --> 00:01:39,200\n",
            "like\n",
            "\n",
            "59\n",
            "00:01:36,640 --> 00:01:40,000\n",
            "collab intro tutorial since many times\n",
            "\n",
            "60\n",
            "00:01:39,200 --> 00:01:41,840\n",
            "you'll be working\n",
            "\n",
            "61\n",
            "00:01:40,000 --> 00:01:43,840\n",
            "with multiple notebooks at a time it can\n",
            "\n",
            "62\n",
            "00:01:41,840 --> 00:01:45,759\n",
            "be really helpful to appropriately name\n",
            "\n",
            "63\n",
            "00:01:43,840 --> 00:01:47,680\n",
            "your notebooks so if you come back to it\n",
            "\n",
            "64\n",
            "00:01:45,759 --> 00:01:48,399\n",
            "months after writing it it will be\n",
            "\n",
            "65\n",
            "00:01:47,680 --> 00:01:50,320\n",
            "descriptive\n",
            "\n",
            "66\n",
            "00:01:48,399 --> 00:01:51,920\n",
            "and much easier for you to know what\n",
            "\n",
            "67\n",
            "00:01:50,320 --> 00:01:53,840\n",
            "what you were doing in that notebook and\n",
            "\n",
            "68\n",
            "00:01:51,920 --> 00:01:55,600\n",
            "everything i do in this notebook will be\n",
            "\n",
            "69\n",
            "00:01:53,840 --> 00:01:57,520\n",
            "saved to my google drive under my google\n",
            "\n",
            "70\n",
            "00:01:55,600 --> 00:01:59,119\n",
            "account so when first we take a look\n",
            "\n",
            "71\n",
            "00:01:57,520 --> 00:02:00,799\n",
            "at google call up the first thing we\n",
            "\n",
            "72\n",
            "00:01:59,119 --> 00:02:01,119\n",
            "will see and the major thing we will see\n",
            "\n",
            "73\n",
            "00:02:00,799 --> 00:02:03,119\n",
            "is\n",
            "\n",
            "74\n",
            "00:02:01,119 --> 00:02:05,040\n",
            "something called a code cell and a play\n",
            "\n",
            "75\n",
            "00:02:03,119 --> 00:02:07,600\n",
            "button we can go ahead and write some\n",
            "\n",
            "76\n",
            "00:02:05,040 --> 00:02:08,959\n",
            "python code here and it will be executed\n",
            "\n",
            "77\n",
            "00:02:07,600 --> 00:02:11,280\n",
            "not on our laptop\n",
            "\n",
            "78\n",
            "00:02:08,959 --> 00:02:12,720\n",
            "but it will be run on google servers and\n",
            "\n",
            "79\n",
            "00:02:11,280 --> 00:02:13,360\n",
            "currently we are not connected at\n",
            "\n",
            "80\n",
            "00:02:12,720 --> 00:02:15,120\n",
            "servers\n",
            "\n",
            "81\n",
            "00:02:13,360 --> 00:02:16,800\n",
            "but we don't have to click here to be\n",
            "\n",
            "82\n",
            "00:02:15,120 --> 00:02:18,800\n",
            "connected but instead\n",
            "\n",
            "83\n",
            "00:02:16,800 --> 00:02:20,160\n",
            "usually what you want to do is to write\n",
            "\n",
            "84\n",
            "00:02:18,800 --> 00:02:22,480\n",
            "some code here and it will be\n",
            "\n",
            "85\n",
            "00:02:20,160 --> 00:02:26,160\n",
            "automatically connect to google servers\n",
            "\n",
            "86\n",
            "00:02:22,480 --> 00:02:26,160\n",
            "so let's write a simple python code\n",
            "\n",
            "87\n",
            "00:02:26,400 --> 00:02:30,239\n",
            "now there are a couple ways i can run\n",
            "\n",
            "88\n",
            "00:02:27,920 --> 00:02:37,120\n",
            "this code first one and the default one\n",
            "\n",
            "89\n",
            "00:02:30,239 --> 00:02:39,120\n",
            "is to click on display button\n",
            "\n",
            "90\n",
            "00:02:37,120 --> 00:02:40,560\n",
            "and we just ran our first python code\n",
            "\n",
            "91\n",
            "00:02:39,120 --> 00:02:43,120\n",
            "usually you'll want to use\n",
            "\n",
            "92\n",
            "00:02:40,560 --> 00:02:44,319\n",
            "keyboard shortcuts and the shortcut to\n",
            "\n",
            "93\n",
            "00:02:43,120 --> 00:02:46,239\n",
            "run a single cell\n",
            "\n",
            "94\n",
            "00:02:44,319 --> 00:02:47,920\n",
            "is command enter which does the same\n",
            "\n",
            "95\n",
            "00:02:46,239 --> 00:02:49,760\n",
            "thing as clicking on that play button\n",
            "\n",
            "96\n",
            "00:02:47,920 --> 00:02:51,120\n",
            "but most of the time chances are you'll\n",
            "\n",
            "97\n",
            "00:02:49,760 --> 00:02:53,200\n",
            "want to run a call cell\n",
            "\n",
            "98\n",
            "00:02:51,120 --> 00:02:55,040\n",
            "and create a new call cell under it so\n",
            "\n",
            "99\n",
            "00:02:53,200 --> 00:02:58,640\n",
            "you can keep coding to do that\n",
            "\n",
            "100\n",
            "00:02:55,040 --> 00:02:58,640\n",
            "we will use shift enter\n",
            "\n",
            "101\n",
            "00:02:59,760 --> 00:03:02,800\n",
            "which i just did notice that there are\n",
            "\n",
            "102\n",
            "00:03:01,519 --> 00:03:04,959\n",
            "two type of cells\n",
            "\n",
            "103\n",
            "00:03:02,800 --> 00:03:06,400\n",
            "i can create one of them is a code cell\n",
            "\n",
            "104\n",
            "00:03:04,959 --> 00:03:08,239\n",
            "and this is where we write our code\n",
            "\n",
            "105\n",
            "00:03:06,400 --> 00:03:10,000\n",
            "another one is a text cell so although\n",
            "\n",
            "106\n",
            "00:03:08,239 --> 00:03:11,680\n",
            "it says text cell\n",
            "\n",
            "107\n",
            "00:03:10,000 --> 00:03:13,920\n",
            "text is not the only thing we can write\n",
            "\n",
            "108\n",
            "00:03:11,680 --> 00:03:16,000\n",
            "here once you write some text\n",
            "\n",
            "109\n",
            "00:03:13,920 --> 00:03:17,040\n",
            "using the icons above we can turn it\n",
            "\n",
            "110\n",
            "00:03:16,000 --> 00:03:19,920\n",
            "into bold\n",
            "\n",
            "111\n",
            "00:03:17,040 --> 00:03:20,480\n",
            "italic title as well as other features\n",
            "\n",
            "112\n",
            "00:03:19,920 --> 00:03:23,360\n",
            "such as\n",
            "\n",
            "113\n",
            "00:03:20,480 --> 00:03:25,200\n",
            "images or videos lists and bullet points\n",
            "\n",
            "114\n",
            "00:03:23,360 --> 00:03:25,760\n",
            "and there's actually even a language for\n",
            "\n",
            "115\n",
            "00:03:25,200 --> 00:03:28,400\n",
            "that called\n",
            "\n",
            "116\n",
            "00:03:25,760 --> 00:03:30,319\n",
            "markdown language and as you're getting\n",
            "\n",
            "117\n",
            "00:03:28,400 --> 00:03:31,040\n",
            "started with collab it's not necessary\n",
            "\n",
            "118\n",
            "00:03:30,319 --> 00:03:32,640\n",
            "to learn\n",
            "\n",
            "119\n",
            "00:03:31,040 --> 00:03:34,239\n",
            "markdown language but if you are\n",
            "\n",
            "120\n",
            "00:03:32,640 --> 00:03:36,080\n",
            "interested you can do that\n",
            "\n",
            "121\n",
            "00:03:34,239 --> 00:03:38,000\n",
            "especially later on in your artificial\n",
            "\n",
            "122\n",
            "00:03:36,080 --> 00:03:39,760\n",
            "intelligence or data science journey\n",
            "\n",
            "123\n",
            "00:03:38,000 --> 00:03:41,920\n",
            "for now what you should know is that we\n",
            "\n",
            "124\n",
            "00:03:39,760 --> 00:03:42,879\n",
            "can create code cells that are modular\n",
            "\n",
            "125\n",
            "00:03:41,920 --> 00:03:45,360\n",
            "and we can also\n",
            "\n",
            "126\n",
            "00:03:42,879 --> 00:03:48,080\n",
            "use text cells to make more explanations\n",
            "\n",
            "127\n",
            "00:03:45,360 --> 00:03:50,080\n",
            "or display images that will be helpful\n",
            "\n",
            "128\n",
            "00:03:48,080 --> 00:03:52,159\n",
            "and maybe explain our code better with\n",
            "\n",
            "129\n",
            "00:03:50,080 --> 00:03:53,120\n",
            "some rich text formats so when we write\n",
            "\n",
            "130\n",
            "00:03:52,159 --> 00:03:55,200\n",
            "code inside\n",
            "\n",
            "131\n",
            "00:03:53,120 --> 00:03:56,400\n",
            "code cells we said that it's not running\n",
            "\n",
            "132\n",
            "00:03:55,200 --> 00:03:58,480\n",
            "in our own laptop but\n",
            "\n",
            "133\n",
            "00:03:56,400 --> 00:04:00,239\n",
            "it's running at google servers and there\n",
            "\n",
            "134\n",
            "00:03:58,480 --> 00:04:01,680\n",
            "are actually three types of servers that\n",
            "\n",
            "135\n",
            "00:04:00,239 --> 00:04:03,200\n",
            "google can allocate to us\n",
            "\n",
            "136\n",
            "00:04:01,680 --> 00:04:04,720\n",
            "and we can change it from here if you go\n",
            "\n",
            "137\n",
            "00:04:03,200 --> 00:04:06,720\n",
            "to runtime and\n",
            "\n",
            "138\n",
            "00:04:04,720 --> 00:04:08,879\n",
            "change runtime type you will see that\n",
            "\n",
            "139\n",
            "00:04:06,720 --> 00:04:10,959\n",
            "you have the option to have a hardware\n",
            "\n",
            "140\n",
            "00:04:08,879 --> 00:04:13,040\n",
            "accelerator right now by default you are\n",
            "\n",
            "141\n",
            "00:04:10,959 --> 00:04:13,760\n",
            "not given a hardware accelerator but if\n",
            "\n",
            "142\n",
            "00:04:13,040 --> 00:04:16,239\n",
            "you need one\n",
            "\n",
            "143\n",
            "00:04:13,760 --> 00:04:16,880\n",
            "it's possible to have either a gpu or a\n",
            "\n",
            "144\n",
            "00:04:16,239 --> 00:04:18,400\n",
            "tpu\n",
            "\n",
            "145\n",
            "00:04:16,880 --> 00:04:20,560\n",
            "so why would you need them if you're\n",
            "\n",
            "146\n",
            "00:04:18,400 --> 00:04:21,280\n",
            "just getting started and writing python\n",
            "\n",
            "147\n",
            "00:04:20,560 --> 00:04:23,440\n",
            "code or\n",
            "\n",
            "148\n",
            "00:04:21,280 --> 00:04:25,440\n",
            "simple machine learning code that runs\n",
            "\n",
            "149\n",
            "00:04:23,440 --> 00:04:26,720\n",
            "fast enough most likely you'll be fine\n",
            "\n",
            "150\n",
            "00:04:25,440 --> 00:04:28,720\n",
            "with the default version\n",
            "\n",
            "151\n",
            "00:04:26,720 --> 00:04:29,919\n",
            "which has no hardware accelerator\n",
            "\n",
            "152\n",
            "00:04:28,720 --> 00:04:32,080\n",
            "meaning that it will run\n",
            "\n",
            "153\n",
            "00:04:29,919 --> 00:04:34,080\n",
            "on cpus but as your artificial\n",
            "\n",
            "154\n",
            "00:04:32,080 --> 00:04:36,080\n",
            "intelligence code gets larger and more\n",
            "\n",
            "155\n",
            "00:04:34,080 --> 00:04:38,400\n",
            "complex as you do more operations that\n",
            "\n",
            "156\n",
            "00:04:36,080 --> 00:04:40,160\n",
            "operations you make will take more time\n",
            "\n",
            "157\n",
            "00:04:38,400 --> 00:04:41,440\n",
            "and you'll want to use some kind of\n",
            "\n",
            "158\n",
            "00:04:40,160 --> 00:04:43,840\n",
            "hardware accelerator\n",
            "\n",
            "159\n",
            "00:04:41,440 --> 00:04:45,840\n",
            "and the most common hardware accelerator\n",
            "\n",
            "160\n",
            "00:04:43,840 --> 00:04:48,560\n",
            "is a gpu\n",
            "\n",
            "161\n",
            "00:04:45,840 --> 00:04:49,120\n",
            "and we have access to free gpus within\n",
            "\n",
            "162\n",
            "00:04:48,560 --> 00:04:50,639\n",
            "collapse\n",
            "\n",
            "163\n",
            "00:04:49,120 --> 00:04:52,639\n",
            "one thing you will notice is that when\n",
            "\n",
            "164\n",
            "00:04:50,639 --> 00:04:54,479\n",
            "you change the hardware accelerator type\n",
            "\n",
            "165\n",
            "00:04:52,639 --> 00:04:57,919\n",
            "from num to gpu\n",
            "\n",
            "166\n",
            "00:04:54,479 --> 00:05:00,000\n",
            "or a tpu it will basically tell you that\n",
            "\n",
            "167\n",
            "00:04:57,919 --> 00:05:01,759\n",
            "if you don't need a hardware accelerator\n",
            "\n",
            "168\n",
            "00:05:00,000 --> 00:05:04,000\n",
            "do not use a hardware accelerator\n",
            "\n",
            "169\n",
            "00:05:01,759 --> 00:05:05,680\n",
            "and this way google can allocate its\n",
            "\n",
            "170\n",
            "00:05:04,000 --> 00:05:07,840\n",
            "gpus or tpus\n",
            "\n",
            "171\n",
            "00:05:05,680 --> 00:05:10,000\n",
            "to people who need more computational\n",
            "\n",
            "172\n",
            "00:05:07,840 --> 00:05:12,160\n",
            "power for now i'm just writing simple\n",
            "\n",
            "173\n",
            "00:05:10,000 --> 00:05:13,520\n",
            "python code so i can change it to none\n",
            "\n",
            "174\n",
            "00:05:12,160 --> 00:05:15,840\n",
            "one thing you should know is that when\n",
            "\n",
            "175\n",
            "00:05:13,520 --> 00:05:17,759\n",
            "you leave your notebook unused for hours\n",
            "\n",
            "176\n",
            "00:05:15,840 --> 00:05:19,520\n",
            "your notebook will be disconnected from\n",
            "\n",
            "177\n",
            "00:05:17,759 --> 00:05:20,080\n",
            "google servers to more efficiently\n",
            "\n",
            "178\n",
            "00:05:19,520 --> 00:05:22,320\n",
            "allocate\n",
            "\n",
            "179\n",
            "00:05:20,080 --> 00:05:24,000\n",
            "its resources but do not worry when that\n",
            "\n",
            "180\n",
            "00:05:22,320 --> 00:05:26,240\n",
            "happens all of your code will be\n",
            "\n",
            "181\n",
            "00:05:24,000 --> 00:05:28,240\n",
            "still safe and nothing changes on them\n",
            "\n",
            "182\n",
            "00:05:26,240 --> 00:05:29,840\n",
            "but you will need to rerun your code and\n",
            "\n",
            "183\n",
            "00:05:28,240 --> 00:05:31,039\n",
            "to do that you can also do it from\n",
            "\n",
            "184\n",
            "00:05:29,840 --> 00:05:34,880\n",
            "runtime\n",
            "\n",
            "185\n",
            "00:05:31,039 --> 00:05:36,560\n",
            "and run all so we talked about cpus and\n",
            "\n",
            "186\n",
            "00:05:34,880 --> 00:05:38,560\n",
            "gpus that you can connect to\n",
            "\n",
            "187\n",
            "00:05:36,560 --> 00:05:40,080\n",
            "but there's also another one which we\n",
            "\n",
            "188\n",
            "00:05:38,560 --> 00:05:41,680\n",
            "can access from\n",
            "\n",
            "189\n",
            "00:05:40,080 --> 00:05:43,600\n",
            "change runtime type and hardware\n",
            "\n",
            "190\n",
            "00:05:41,680 --> 00:05:47,440\n",
            "accelerator and that was\n",
            "\n",
            "191\n",
            "00:05:43,600 --> 00:05:49,600\n",
            "tpu and tpus are tensor processing units\n",
            "\n",
            "192\n",
            "00:05:47,440 --> 00:05:51,120\n",
            "and do not be scared by the name these\n",
            "\n",
            "193\n",
            "00:05:49,600 --> 00:05:53,440\n",
            "are basically processing\n",
            "\n",
            "194\n",
            "00:05:51,120 --> 00:05:54,720\n",
            "units or processing chips that are\n",
            "\n",
            "195\n",
            "00:05:53,440 --> 00:05:56,800\n",
            "specifically designed\n",
            "\n",
            "196\n",
            "00:05:54,720 --> 00:05:58,400\n",
            "for significantly accelerating\n",
            "\n",
            "197\n",
            "00:05:56,800 --> 00:06:00,560\n",
            "artificial intelligence code\n",
            "\n",
            "198\n",
            "00:05:58,400 --> 00:06:01,759\n",
            "and we also have free access to them\n",
            "\n",
            "199\n",
            "00:06:00,560 --> 00:06:04,080\n",
            "from cola\n",
            "\n",
            "200\n",
            "00:06:01,759 --> 00:06:06,240\n",
            "and this can be especially useful if you\n",
            "\n",
            "201\n",
            "00:06:04,080 --> 00:06:08,639\n",
            "have larger machine learning models\n",
            "\n",
            "202\n",
            "00:06:06,240 --> 00:06:09,280\n",
            "or artificial intelligence models but if\n",
            "\n",
            "203\n",
            "00:06:08,639 --> 00:06:11,199\n",
            "you are using\n",
            "\n",
            "204\n",
            "00:06:09,280 --> 00:06:12,800\n",
            "a smaller model you will be most likely\n",
            "\n",
            "205\n",
            "00:06:11,199 --> 00:06:16,000\n",
            "fine with a gpu\n",
            "\n",
            "206\n",
            "00:06:12,800 --> 00:06:18,160\n",
            "or none hardware accelerator\n",
            "\n",
            "207\n",
            "00:06:16,000 --> 00:06:19,919\n",
            "so until now we just seen python code\n",
            "\n",
            "208\n",
            "00:06:18,160 --> 00:06:21,280\n",
            "but we didn't see anything about data\n",
            "\n",
            "209\n",
            "00:06:19,919 --> 00:06:23,120\n",
            "science machine learning or\n",
            "\n",
            "210\n",
            "00:06:21,280 --> 00:06:25,120\n",
            "artificial intelligence code but one of\n",
            "\n",
            "211\n",
            "00:06:23,120 --> 00:06:27,360\n",
            "the major advantages of using google\n",
            "\n",
            "212\n",
            "00:06:25,120 --> 00:06:28,560\n",
            "colab is that you can't only code python\n",
            "\n",
            "213\n",
            "00:06:27,360 --> 00:06:30,720\n",
            "code in the browser\n",
            "\n",
            "214\n",
            "00:06:28,560 --> 00:06:32,800\n",
            "but all of the major data science and\n",
            "\n",
            "215\n",
            "00:06:30,720 --> 00:06:34,240\n",
            "artificial intelligence libraries and\n",
            "\n",
            "216\n",
            "00:06:32,800 --> 00:06:36,400\n",
            "tools come already\n",
            "\n",
            "217\n",
            "00:06:34,240 --> 00:06:37,280\n",
            "pre-installed in collab and we can start\n",
            "\n",
            "218\n",
            "00:06:36,400 --> 00:06:40,960\n",
            "using them\n",
            "\n",
            "219\n",
            "00:06:37,280 --> 00:06:40,960\n",
            "with import statements such as\n",
            "\n",
            "220\n",
            "00:06:45,039 --> 00:06:48,400\n",
            "so in order to use tensorflow numpy\n",
            "\n",
            "221\n",
            "00:06:47,759 --> 00:06:50,880\n",
            "pandas\n",
            "\n",
            "222\n",
            "00:06:48,400 --> 00:06:52,720\n",
            "matbaltip and pytorch and many more of\n",
            "\n",
            "223\n",
            "00:06:50,880 --> 00:06:53,599\n",
            "them since they're already pre-installed\n",
            "\n",
            "224\n",
            "00:06:52,720 --> 00:06:55,520\n",
            "for us we can\n",
            "\n",
            "225\n",
            "00:06:53,599 --> 00:06:57,440\n",
            "start using them with these import\n",
            "\n",
            "226\n",
            "00:06:55,520 --> 00:07:02,800\n",
            "statements and if you want we can also\n",
            "\n",
            "227\n",
            "00:06:57,440 --> 00:07:04,639\n",
            "check their versions and as you can see\n",
            "\n",
            "228\n",
            "00:07:02,800 --> 00:07:06,639\n",
            "we're using the latest versions\n",
            "\n",
            "229\n",
            "00:07:04,639 --> 00:07:08,160\n",
            "of the libraries such as using\n",
            "\n",
            "230\n",
            "00:07:06,639 --> 00:07:10,000\n",
            "tensorflow 2.4\n",
            "\n",
            "231\n",
            "00:07:08,160 --> 00:07:11,919\n",
            "which is the latest one as of right now\n",
            "\n",
            "232\n",
            "00:07:10,000 --> 00:07:14,080\n",
            "and if you're not familiar with\n",
            "\n",
            "233\n",
            "00:07:11,919 --> 00:07:15,840\n",
            "any of these do not worry as you get\n",
            "\n",
            "234\n",
            "00:07:14,080 --> 00:07:16,479\n",
            "started with colab it will be much\n",
            "\n",
            "235\n",
            "00:07:15,840 --> 00:07:18,479\n",
            "easier to\n",
            "\n",
            "236\n",
            "00:07:16,479 --> 00:07:19,599\n",
            "learn them as you're coding them so as\n",
            "\n",
            "237\n",
            "00:07:18,479 --> 00:07:21,759\n",
            "you are getting started you are not\n",
            "\n",
            "238\n",
            "00:07:19,599 --> 00:07:23,759\n",
            "required to know what this code does or\n",
            "\n",
            "239\n",
            "00:07:21,759 --> 00:07:25,759\n",
            "whether these libraries are about and\n",
            "\n",
            "240\n",
            "00:07:23,759 --> 00:07:27,680\n",
            "you can learn them one video at a time\n",
            "\n",
            "241\n",
            "00:07:25,759 --> 00:07:28,880\n",
            "and every now and then you will want to\n",
            "\n",
            "242\n",
            "00:07:27,680 --> 00:07:31,039\n",
            "use libraries\n",
            "\n",
            "243\n",
            "00:07:28,880 --> 00:07:32,639\n",
            "that are not inside collab and when you\n",
            "\n",
            "244\n",
            "00:07:31,039 --> 00:07:34,639\n",
            "want to do that you will want to\n",
            "\n",
            "245\n",
            "00:07:32,639 --> 00:07:36,960\n",
            "prefix your code with an exclamation\n",
            "\n",
            "246\n",
            "00:07:34,639 --> 00:07:36,960\n",
            "mark\n",
            "\n",
            "247\n",
            "00:07:37,360 --> 00:07:41,120\n",
            "and with that you can install other\n",
            "\n",
            "248\n",
            "00:07:39,199 --> 00:07:42,880\n",
            "libraries to call up as well\n",
            "\n",
            "249\n",
            "00:07:41,120 --> 00:07:44,160\n",
            "and when you want to use them you can\n",
            "\n",
            "250\n",
            "00:07:42,880 --> 00:07:47,360\n",
            "start using them with\n",
            "\n",
            "251\n",
            "00:07:44,160 --> 00:07:48,639\n",
            "import statements as well so as you are\n",
            "\n",
            "252\n",
            "00:07:47,360 --> 00:07:50,160\n",
            "working within a notebook\n",
            "\n",
            "253\n",
            "00:07:48,639 --> 00:07:51,840\n",
            "if you want to create a new notebook you\n",
            "\n",
            "254\n",
            "00:07:50,160 --> 00:07:54,000\n",
            "can do that from file\n",
            "\n",
            "255\n",
            "00:07:51,840 --> 00:07:55,599\n",
            "and new notebook and similar notebook\n",
            "\n",
            "256\n",
            "00:07:54,000 --> 00:07:56,160\n",
            "actions such as opening the existing\n",
            "\n",
            "257\n",
            "00:07:55,599 --> 00:07:58,319\n",
            "ones\n",
            "\n",
            "258\n",
            "00:07:56,160 --> 00:07:59,280\n",
            "you can also do them from here as well a\n",
            "\n",
            "259\n",
            "00:07:58,319 --> 00:08:01,120\n",
            "lot of the times\n",
            "\n",
            "260\n",
            "00:07:59,280 --> 00:08:02,319\n",
            "many of the machine learning data\n",
            "\n",
            "261\n",
            "00:08:01,120 --> 00:08:04,080\n",
            "science or\n",
            "\n",
            "262\n",
            "00:08:02,319 --> 00:08:05,599\n",
            "artificial intelligence codes are\n",
            "\n",
            "263\n",
            "00:08:04,080 --> 00:08:07,280\n",
            "written in collab and\n",
            "\n",
            "264\n",
            "00:08:05,599 --> 00:08:09,360\n",
            "when you're learning from them one thing\n",
            "\n",
            "265\n",
            "00:08:07,280 --> 00:08:11,360\n",
            "you want to do is to save a copy\n",
            "\n",
            "266\n",
            "00:08:09,360 --> 00:08:13,280\n",
            "in drive and when you save a copy and\n",
            "\n",
            "267\n",
            "00:08:11,360 --> 00:08:14,160\n",
            "drive you can have a copy of the\n",
            "\n",
            "268\n",
            "00:08:13,280 --> 00:08:16,080\n",
            "tutorial\n",
            "\n",
            "269\n",
            "00:08:14,160 --> 00:08:18,319\n",
            "and save it under your google drive as\n",
            "\n",
            "270\n",
            "00:08:16,080 --> 00:08:18,319\n",
            "well\n",
            "\n",
            "271\n",
            "00:08:18,400 --> 00:08:23,120\n",
            "and if i do that i will have all the\n",
            "\n",
            "272\n",
            "00:08:20,160 --> 00:08:25,280\n",
            "code from that previous notebook\n",
            "\n",
            "273\n",
            "00:08:23,120 --> 00:08:27,039\n",
            "except in this one i'm not connected\n",
            "\n",
            "274\n",
            "00:08:25,280 --> 00:08:28,560\n",
            "which i can connect in a second as well\n",
            "\n",
            "275\n",
            "00:08:27,039 --> 00:08:32,240\n",
            "and i will have this title change\n",
            "\n",
            "276\n",
            "00:08:28,560 --> 00:08:35,120\n",
            "to a copy of follow-up intro tutorial\n",
            "\n",
            "277\n",
            "00:08:32,240 --> 00:08:36,959\n",
            "from collab info tutorial and if i wanna\n",
            "\n",
            "278\n",
            "00:08:35,120 --> 00:08:37,360\n",
            "run this code you can either run that\n",
            "\n",
            "279\n",
            "00:08:36,959 --> 00:08:40,640\n",
            "code\n",
            "\n",
            "280\n",
            "00:08:37,360 --> 00:08:45,120\n",
            "cell by cell or you can\n",
            "\n",
            "281\n",
            "00:08:40,640 --> 00:08:46,880\n",
            "run that from runtime and run all\n",
            "\n",
            "282\n",
            "00:08:45,120 --> 00:08:49,440\n",
            "which will run all the cells\n",
            "\n",
            "283\n",
            "00:08:46,880 --> 00:08:51,519\n",
            "sequentially\n",
            "\n",
            "284\n",
            "00:08:49,440 --> 00:08:52,720\n",
            "and that's one of the reason why collab\n",
            "\n",
            "285\n",
            "00:08:51,519 --> 00:08:54,080\n",
            "is so popular among\n",
            "\n",
            "286\n",
            "00:08:52,720 --> 00:08:55,839\n",
            "data science and machine learning\n",
            "\n",
            "287\n",
            "00:08:54,080 --> 00:08:56,800\n",
            "communities and you can find ldap\n",
            "\n",
            "288\n",
            "00:08:55,839 --> 00:08:59,120\n",
            "tutorials\n",
            "\n",
            "289\n",
            "00:08:56,800 --> 00:09:00,800\n",
            "are written in collab for ease of\n",
            "\n",
            "290\n",
            "00:08:59,120 --> 00:09:02,240\n",
            "shareability because you can literally\n",
            "\n",
            "291\n",
            "00:09:00,800 --> 00:09:04,240\n",
            "share them with a single\n",
            "\n",
            "292\n",
            "00:09:02,240 --> 00:09:06,080\n",
            "link and everyone that has an internet\n",
            "\n",
            "293\n",
            "00:09:04,240 --> 00:09:08,399\n",
            "connection and the google account\n",
            "\n",
            "294\n",
            "00:09:06,080 --> 00:09:09,760\n",
            "can have access to that code and run\n",
            "\n",
            "295\n",
            "00:09:08,399 --> 00:09:12,880\n",
            "them themselves\n",
            "\n",
            "296\n",
            "00:09:09,760 --> 00:09:14,720\n",
            "as well as modifying it\n",
            "\n",
            "297\n",
            "00:09:12,880 --> 00:09:18,320\n",
            "and if you want to download the notebook\n",
            "\n",
            "298\n",
            "00:09:14,720 --> 00:09:20,640\n",
            "you can also do that from here\n",
            "\n",
            "299\n",
            "00:09:18,320 --> 00:09:23,279\n",
            "under file you can go download this\n",
            "\n",
            "300\n",
            "00:09:20,640 --> 00:09:26,000\n",
            "notebook as a ipnb file\n",
            "\n",
            "301\n",
            "00:09:23,279 --> 00:09:28,320\n",
            "which is what we are using here and you\n",
            "\n",
            "302\n",
            "00:09:26,000 --> 00:09:29,760\n",
            "can also download it as a python file\n",
            "\n",
            "303\n",
            "00:09:28,320 --> 00:09:31,920\n",
            "one thing you should know is that\n",
            "\n",
            "304\n",
            "00:09:29,760 --> 00:09:34,560\n",
            "currently we're working with the ipad b\n",
            "\n",
            "305\n",
            "00:09:31,920 --> 00:09:36,720\n",
            "file and this is the same file format\n",
            "\n",
            "306\n",
            "00:09:34,560 --> 00:09:39,040\n",
            "that you use in jupyter notebooks\n",
            "\n",
            "307\n",
            "00:09:36,720 --> 00:09:41,040\n",
            "and ipamb files are more popular with\n",
            "\n",
            "308\n",
            "00:09:39,040 --> 00:09:42,320\n",
            "data science and machine learning\n",
            "\n",
            "309\n",
            "00:09:41,040 --> 00:09:44,720\n",
            "because they have additional\n",
            "\n",
            "310\n",
            "00:09:42,320 --> 00:09:45,600\n",
            "capabilities on top of python files some\n",
            "\n",
            "311\n",
            "00:09:44,720 --> 00:09:47,839\n",
            "of those features\n",
            "\n",
            "312\n",
            "00:09:45,600 --> 00:09:49,600\n",
            "are called cells which you can save your\n",
            "\n",
            "313\n",
            "00:09:47,839 --> 00:09:51,839\n",
            "outputs of the code cells\n",
            "\n",
            "314\n",
            "00:09:49,600 --> 00:09:53,680\n",
            "you can also change the order of the\n",
            "\n",
            "315\n",
            "00:09:51,839 --> 00:09:56,160\n",
            "code cells more easily\n",
            "\n",
            "316\n",
            "00:09:53,680 --> 00:09:56,800\n",
            "from these up or down arrows you can\n",
            "\n",
            "317\n",
            "00:09:56,160 --> 00:09:58,959\n",
            "delete them\n",
            "\n",
            "318\n",
            "00:09:56,800 --> 00:10:00,399\n",
            "comment them and they're overall in a\n",
            "\n",
            "319\n",
            "00:09:58,959 --> 00:10:02,399\n",
            "more modular structure\n",
            "\n",
            "320\n",
            "00:10:00,399 --> 00:10:04,480\n",
            "that are a better fit for data science\n",
            "\n",
            "321\n",
            "00:10:02,399 --> 00:10:06,160\n",
            "and machine learning code so these were\n",
            "\n",
            "322\n",
            "00:10:04,480 --> 00:10:07,279\n",
            "everything you needed to get started\n",
            "\n",
            "323\n",
            "00:10:06,160 --> 00:10:09,200\n",
            "with google collab\n",
            "\n",
            "324\n",
            "00:10:07,279 --> 00:10:10,560\n",
            "as you just have seen you can just\n",
            "\n",
            "325\n",
            "00:10:09,200 --> 00:10:12,959\n",
            "search for google call up\n",
            "\n",
            "326\n",
            "00:10:10,560 --> 00:10:14,000\n",
            "and get coding within seconds without\n",
            "\n",
            "327\n",
            "00:10:12,959 --> 00:10:15,600\n",
            "any setup\n",
            "\n",
            "328\n",
            "00:10:14,000 --> 00:10:17,200\n",
            "and everything you do will be saved\n",
            "\n",
            "329\n",
            "00:10:15,600 --> 00:10:19,040\n",
            "under your google account and you can\n",
            "\n",
            "330\n",
            "00:10:17,200 --> 00:10:20,800\n",
            "reach them from your google drive\n",
            "\n",
            "331\n",
            "00:10:19,040 --> 00:10:22,320\n",
            "and share them with a single link so\n",
            "\n",
            "332\n",
            "00:10:20,800 --> 00:10:23,839\n",
            "this was it for this tutorial\n",
            "\n",
            "333\n",
            "00:10:22,320 --> 00:10:26,000\n",
            "if you learned better by reading by the\n",
            "\n",
            "334\n",
            "00:10:23,839 --> 00:10:26,959\n",
            "way i also have this tutorial in article\n",
            "\n",
            "335\n",
            "00:10:26,000 --> 00:10:28,880\n",
            "format as well\n",
            "\n",
            "336\n",
            "00:10:26,959 --> 00:10:30,800\n",
            "and i will make sure to put a link to it\n",
            "\n",
            "337\n",
            "00:10:28,880 --> 00:10:32,399\n",
            "in the description box with that i hope\n",
            "\n",
            "338\n",
            "00:10:30,800 --> 00:10:34,000\n",
            "you got some value out of this video if\n",
            "\n",
            "339\n",
            "00:10:32,399 --> 00:10:35,760\n",
            "you want to see more machine learning\n",
            "\n",
            "340\n",
            "00:10:34,000 --> 00:10:37,760\n",
            "and artificial intelligence tutorials\n",
            "\n",
            "341\n",
            "00:10:35,760 --> 00:10:40,480\n",
            "make sure to like and subscribe and i\n",
            "\n",
            "342\n",
            "00:10:37,760 --> 00:10:40,480\n",
            "will see you next time\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCH7e5srPeIW"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEElKekZSYlk",
        "outputId": "583d5956-0724-4f04-91b8-a0e6b087ac7a"
      },
      "source": [
        "nltk.download()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue:  \n",
            "Hit Enter to continue: \n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "Hit Enter to continue: \n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "Hit Enter to continue: \n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> \n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> all\n",
            "    Downloading collection 'all'\n",
            "       | \n",
            "       | Downloading package abc to /root/nltk_data...\n",
            "       |   Unzipping corpora/abc.zip.\n",
            "       | Downloading package alpino to /root/nltk_data...\n",
            "       |   Unzipping corpora/alpino.zip.\n",
            "       | Downloading package biocreative_ppi to /root/nltk_data...\n",
            "       |   Unzipping corpora/biocreative_ppi.zip.\n",
            "       | Downloading package brown to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown.zip.\n",
            "       | Downloading package brown_tei to /root/nltk_data...\n",
            "       |   Unzipping corpora/brown_tei.zip.\n",
            "       | Downloading package cess_cat to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_cat.zip.\n",
            "       | Downloading package cess_esp to /root/nltk_data...\n",
            "       |   Unzipping corpora/cess_esp.zip.\n",
            "       | Downloading package chat80 to /root/nltk_data...\n",
            "       |   Unzipping corpora/chat80.zip.\n",
            "       | Downloading package city_database to /root/nltk_data...\n",
            "       |   Unzipping corpora/city_database.zip.\n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package comparative_sentences to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/comparative_sentences.zip.\n",
            "       | Downloading package comtrans to /root/nltk_data...\n",
            "       | Downloading package conll2000 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2000.zip.\n",
            "       | Downloading package conll2002 to /root/nltk_data...\n",
            "       |   Unzipping corpora/conll2002.zip.\n",
            "       | Downloading package conll2007 to /root/nltk_data...\n",
            "       | Downloading package crubadan to /root/nltk_data...\n",
            "       |   Unzipping corpora/crubadan.zip.\n",
            "       | Downloading package dependency_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/dependency_treebank.zip.\n",
            "       | Downloading package dolch to /root/nltk_data...\n",
            "       |   Unzipping corpora/dolch.zip.\n",
            "       | Downloading package europarl_raw to /root/nltk_data...\n",
            "       |   Unzipping corpora/europarl_raw.zip.\n",
            "       | Downloading package floresta to /root/nltk_data...\n",
            "       |   Unzipping corpora/floresta.zip.\n",
            "       | Downloading package framenet_v15 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v15.zip.\n",
            "       | Downloading package framenet_v17 to /root/nltk_data...\n",
            "       |   Unzipping corpora/framenet_v17.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package ieer to /root/nltk_data...\n",
            "       |   Unzipping corpora/ieer.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package indian to /root/nltk_data...\n",
            "       |   Unzipping corpora/indian.zip.\n",
            "       | Downloading package jeita to /root/nltk_data...\n",
            "       | Downloading package kimmo to /root/nltk_data...\n",
            "       |   Unzipping corpora/kimmo.zip.\n",
            "       | Downloading package knbc to /root/nltk_data...\n",
            "       | Downloading package lin_thesaurus to /root/nltk_data...\n",
            "       |   Unzipping corpora/lin_thesaurus.zip.\n",
            "       | Downloading package mac_morpho to /root/nltk_data...\n",
            "       |   Unzipping corpora/mac_morpho.zip.\n",
            "       | Downloading package machado to /root/nltk_data...\n",
            "       | Downloading package masc_tagged to /root/nltk_data...\n",
            "       | Downloading package moses_sample to /root/nltk_data...\n",
            "       |   Unzipping models/moses_sample.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "       | Downloading package nps_chat to /root/nltk_data...\n",
            "       |   Unzipping corpora/nps_chat.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       |   Unzipping corpora/omw.zip.\n",
            "       | Downloading package opinion_lexicon to /root/nltk_data...\n",
            "       |   Unzipping corpora/opinion_lexicon.zip.\n",
            "       | Downloading package paradigms to /root/nltk_data...\n",
            "       |   Unzipping corpora/paradigms.zip.\n",
            "       | Downloading package pil to /root/nltk_data...\n",
            "       |   Unzipping corpora/pil.zip.\n",
            "       | Downloading package pl196x to /root/nltk_data...\n",
            "       |   Unzipping corpora/pl196x.zip.\n",
            "       | Downloading package ppattach to /root/nltk_data...\n",
            "       |   Unzipping corpora/ppattach.zip.\n",
            "       | Downloading package problem_reports to /root/nltk_data...\n",
            "       |   Unzipping corpora/problem_reports.zip.\n",
            "       | Downloading package propbank to /root/nltk_data...\n",
            "       | Downloading package ptb to /root/nltk_data...\n",
            "       |   Unzipping corpora/ptb.zip.\n",
            "       | Downloading package product_reviews_1 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_1.zip.\n",
            "       | Downloading package product_reviews_2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/product_reviews_2.zip.\n",
            "       | Downloading package pros_cons to /root/nltk_data...\n",
            "       |   Unzipping corpora/pros_cons.zip.\n",
            "       | Downloading package qc to /root/nltk_data...\n",
            "       |   Unzipping corpora/qc.zip.\n",
            "       | Downloading package reuters to /root/nltk_data...\n",
            "       | Downloading package rte to /root/nltk_data...\n",
            "       |   Unzipping corpora/rte.zip.\n",
            "       | Downloading package semcor to /root/nltk_data...\n",
            "       | Downloading package senseval to /root/nltk_data...\n",
            "       |   Unzipping corpora/senseval.zip.\n",
            "       | Downloading package sentiwordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentiwordnet.zip.\n",
            "       | Downloading package sentence_polarity to /root/nltk_data...\n",
            "       |   Unzipping corpora/sentence_polarity.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package sinica_treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/sinica_treebank.zip.\n",
            "       | Downloading package smultron to /root/nltk_data...\n",
            "       |   Unzipping corpora/smultron.zip.\n",
            "       | Downloading package state_union to /root/nltk_data...\n",
            "       |   Unzipping corpora/state_union.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package subjectivity to /root/nltk_data...\n",
            "       |   Unzipping corpora/subjectivity.zip.\n",
            "       | Downloading package swadesh to /root/nltk_data...\n",
            "       |   Unzipping corpora/swadesh.zip.\n",
            "       | Downloading package switchboard to /root/nltk_data...\n",
            "       |   Unzipping corpora/switchboard.zip.\n",
            "       | Downloading package timit to /root/nltk_data...\n",
            "       |   Unzipping corpora/timit.zip.\n",
            "       | Downloading package toolbox to /root/nltk_data...\n",
            "       |   Unzipping corpora/toolbox.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package udhr to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr.zip.\n",
            "       | Downloading package udhr2 to /root/nltk_data...\n",
            "       |   Unzipping corpora/udhr2.zip.\n",
            "       | Downloading package unicode_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/unicode_samples.zip.\n",
            "       | Downloading package universal_treebanks_v20 to\n",
            "       |     /root/nltk_data...\n",
            "       | Downloading package verbnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet.zip.\n",
            "       | Downloading package verbnet3 to /root/nltk_data...\n",
            "       |   Unzipping corpora/verbnet3.zip.\n",
            "       | Downloading package webtext to /root/nltk_data...\n",
            "       |   Unzipping corpora/webtext.zip.\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet.zip.\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package ycoe to /root/nltk_data...\n",
            "       |   Unzipping corpora/ycoe.zip.\n",
            "       | Downloading package rslp to /root/nltk_data...\n",
            "       |   Unzipping stemmers/rslp.zip.\n",
            "       | Downloading package maxent_treebank_pos_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "       | Downloading package universal_tagset to /root/nltk_data...\n",
            "       |   Unzipping taggers/universal_tagset.zip.\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package book_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/book_grammars.zip.\n",
            "       | Downloading package sample_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/sample_grammars.zip.\n",
            "       | Downloading package spanish_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/spanish_grammars.zip.\n",
            "       | Downloading package basque_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/basque_grammars.zip.\n",
            "       | Downloading package large_grammars to /root/nltk_data...\n",
            "       |   Unzipping grammars/large_grammars.zip.\n",
            "       | Downloading package tagsets to /root/nltk_data...\n",
            "       |   Unzipping help/tagsets.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package bllip_wsj_no_aux to /root/nltk_data...\n",
            "       |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "       | Downloading package word2vec_sample to /root/nltk_data...\n",
            "       |   Unzipping models/word2vec_sample.zip.\n",
            "       | Downloading package panlex_swadesh to /root/nltk_data...\n",
            "       | Downloading package mte_teip5 to /root/nltk_data...\n",
            "       |   Unzipping corpora/mte_teip5.zip.\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | Downloading package averaged_perceptron_tagger_ru to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger_ru.zip.\n",
            "       | Downloading package perluniprops to /root/nltk_data...\n",
            "       |   Unzipping misc/perluniprops.zip.\n",
            "       | Downloading package nonbreaking_prefixes to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "       | Downloading package vader_lexicon to /root/nltk_data...\n",
            "       | Downloading package porter_test to /root/nltk_data...\n",
            "       |   Unzipping stemmers/porter_test.zip.\n",
            "       | Downloading package wmt15_eval to /root/nltk_data...\n",
            "       |   Unzipping models/wmt15_eval.zip.\n",
            "       | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "       |   Unzipping misc/mwa_ppdb.zip.\n",
            "       | \n",
            "     Done downloading collection all\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dPTz8X6SZtU"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}