{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adc3d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc75df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2')\n",
    "model = BertForQuestionAnswering.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "022709d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "question, text = \"\"\"Who was Jim Henson?\"\"\", \"\"\"e examples, you must install it from source.\"\"\"\n",
    "inputs = tokenizer(question, text, return_tensors='pt')\n",
    "input_ids = tokenizer.encode(question, text)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "outputs = model(**inputs, start_positions=None, end_positions=None)\n",
    "loss = outputs.loss\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n",
    "\n",
    "answer_start = torch.argmax(start_scores)\n",
    "answer_end = torch.argmax(end_scores)\n",
    "answer = ' '.join(tokens[answer_start:answer_end+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d59905c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python 3 . 6 + , and p ##yt ##or ##ch 1 . 1 . 0 + or tensor ##flow 2 . 0 +'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_start = torch.argmax(start_scores)\n",
    "answer_end = torch.argmax(end_scores)\n",
    "answer = ' '.join(tokens[answer_start:answer_end+1])\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c201c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "input_dict = tokenizer(question, text, return_tensors='pt')\n",
    "outputs = model(input_dict)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\n",
    "answer = ' '.join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0]+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c9b28b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "232a6f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2')\n",
    "model = BertForQuestionAnswering.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f47e3dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question, text = \"\"\"how can you install transformers?\"\"\", \"\"\"Transformers is tested on Python 3.6+, and PyTorch 1.1.0+ or TensorFlow 2.0+.\n",
    "\n",
    "You should install ðŸ¤— Transformers in a virtual environment. If youâ€™re unfamiliar with Python virtual environments, check out the user guide. Create a virtual environment with the version of Python youâ€™re going to use and activate it.\n",
    "\n",
    "Now, if you want to use ðŸ¤— Transformers, you can install it with pip. If youâ€™d like to play with the examples, you must install it from source.\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d1c7228",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(question, text, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9f37a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07357de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\n",
    "answer = ' '.join(all_tokens[torch.argmax(start_logits, 1)[0] : torch.argmax(end_logits, 1)[0]+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4361e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d44c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(question, text)\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "loss = outputs.loss\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n",
    "\n",
    "answer_start = torch.argmax(start_scores)\n",
    "answer_end = torch.argmax(end_scores)\n",
    "answer = ' '.join(tokens[answer_start:answer_end+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e29f854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "# import torch\n",
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2')\n",
    "# model = BertForQuestionAnswering.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2')\n",
    "\n",
    "question, text = \"\"\"What have transformers been tested on?\"\"\", \"\"\"Transformers is tested on Python 3.6+, and PyTorch 1.1.0+ or TensorFlow 2.0+.\n",
    "\n",
    "You should install ðŸ¤— Transformers in a virtual environment. If youâ€™re unfamiliar with Python virtual environments, check out the user guide. Create a virtual environment with the version of Python youâ€™re going to use and activate it.\n",
    "\n",
    "Now, if you want to use ðŸ¤— Transformers, you can install it with pip. If youâ€™d like to play with the examples, you must install it from source.\"\"\"\n",
    "\n",
    "\n",
    "input_dict = tokenizer(question, text, return_tensors='pt')\n",
    "outputs = model(**input_dict)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\n",
    "answer = ' '.join(all_tokens[torch.argmax(start_logits, 1)[0] : torch.argmax(end_logits, 1)[0]+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03388eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python 3 . 6 + , and p ##yt ##or ##ch 1 . 1 . 0 + or tensor ##flow 2 . 0 +'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2283aa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForQuestionAnswering\n",
    "import tensorflow as tf\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2')\n",
    "model = TFBertForQuestionAnswering.from_pretrained('deepset/bert-large-uncased-whole-word-masking-squad2')\n",
    "\n",
    "question, text = \"Who was Jim Henson?\", \"Jim Henson was a nice puppet\"\n",
    "input_dict = tokenizer(question, text, return_tensors='tf')\n",
    "outputs = model(input_dict)\n",
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits\n",
    "\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\n",
    "answer = ' '.join(all_tokens[tf.math.argmax(start_logits, 1)[0] : tf.math.argmax(end_logits, 1)[0]+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ab19e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
